# -*- coding: utf-8 -*-
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

Generated by Alembic ${alembic_version} on ${create_date}
Environment: ${environment}

WARNING:
- Always review generated code before committing
- Test migrations in development/staging before production
- Consider data preservation during downgrades

"""
<%!
    import os
    import sys
    from datetime import datetime
    
    # Get environment info
    environment = os.getenv('ENVIRONMENT', 'development')
    
    # Add common imports based on environment
    def get_imports():
        imports = []
        if environment == 'production':
            imports.append('# PRODUCTION MIGRATION - EXTREME CAUTION')
        return '\n'.join(imports)
%>

## ==================== IMPORTS ====================
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from sqlalchemy.sql import text, column, table
from sqlalchemy.exc import SQLAlchemyError
<% 
    # Add conditional imports
    db_type = os.getenv('DB_TYPE', 'postgresql')
    
    if db_type == 'postgresql':
        print("from sqlalchemy.dialects.postgresql import JSONB, UUID, ARRAY")
    elif db_type == 'mysql':
        print("from sqlalchemy.dialects.mysql import JSON, INTEGER, VARCHAR")
%>
import logging
from typing import Optional, List, Dict, Any, Union
from datetime import datetime, timezone
import json
import sys

## ==================== LOGGING ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

## ==================== CONFIGURATION ====================
# Revision identifiers, used by Alembic
revision: str = ${repr(up_revision)}
down_revision: Optional[str] = ${repr(down_revision)}
branch_labels: Optional[Union[str, List[str]]] = ${repr(branch_labels)}
depends_on: Optional[Union[str, List[str]]] = ${repr(depends_on)}

# Migration metadata
MIGRATION_NAME = ${repr(message)}
GENERATED_AT = datetime.now(timezone.utc).isoformat()
ENVIRONMENT = "${environment}"
DB_TYPE = "${db_type}"

## ==================== UTILITY FUNCTIONS ====================
def is_production() -> bool:
    """Check if running in production environment."""
    return ENVIRONMENT.lower() == 'production'

def is_testing() -> bool:
    """Check if running in test environment."""
    return ENVIRONMENT.lower() == 'testing'

def get_connection():
    """Get database connection with proper error handling."""
    try:
        return op.get_bind()
    except Exception as e:
        logger.error(f"Failed to get database connection: {e}")
        raise

def execute_sql(sql: str, params: Optional[Dict] = None) -> Any:
    """Safely execute SQL with logging."""
    connection = get_connection()
    logger.info(f"Executing SQL: {sql[:200]}...")
    
    if params:
        logger.debug(f"Parameters: {params}")
    
    try:
        result = connection.execute(text(sql), params or {})
        logger.info(f"SQL executed successfully")
        return result
    except SQLAlchemyError as e:
        logger.error(f"SQL execution failed: {e}")
        raise

def bulk_insert(table_name: str, data: List[Dict], batch_size: int = 1000) -> int:
    """Insert data in batches to avoid large transactions."""
    if not data:
        logger.warning(f"No data to insert into {table_name}")
        return 0
    
    connection = get_connection()
    table_obj = sa.Table(table_name, sa.MetaData(), autoload_with=connection)
    
    total_inserted = 0
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        try:
            connection.execute(table_obj.insert(), batch)
            total_inserted += len(batch)
            logger.info(f"Inserted batch {i//batch_size + 1}: {len(batch)} rows into {table_name}")
        except Exception as e:
            logger.error(f"Failed to insert batch into {table_name}: {e}")
            raise
    
    logger.info(f"Total inserted into {table_name}: {total_inserted} rows")
    return total_inserted

def table_exists(table_name: str, schema: Optional[str] = None) -> bool:
    """Check if a table exists in the database."""
    connection = get_connection()
    
    if DB_TYPE == 'postgresql':
        query = """
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = :schema 
                AND table_name = :table_name
            )
        """
        params = {'schema': schema or 'public', 'table_name': table_name}
    elif DB_TYPE == 'mysql':
        query = """
            SELECT COUNT(*) > 0 
            FROM information_schema.tables 
            WHERE table_schema = DATABASE() 
            AND table_name = :table_name
        """
        params = {'table_name': table_name}
    else:
        # SQLite
        query = """
            SELECT COUNT(*) > 0 
            FROM sqlite_master 
            WHERE type='table' 
            AND name = :table_name
        """
        params = {'table_name': table_name}
    
    result = execute_sql(query, params)
    exists = result.scalar()
    logger.debug(f"Table {table_name} exists: {exists}")
    return exists

def column_exists(table_name: str, column_name: str, schema: Optional[str] = None) -> bool:
    """Check if a column exists in a table."""
    connection = get_connection()
    
    if DB_TYPE == 'postgresql':
        query = """
            SELECT EXISTS (
                SELECT FROM information_schema.columns 
                WHERE table_schema = :schema 
                AND table_name = :table_name 
                AND column_name = :column_name
            )
        """
        params = {'schema': schema or 'public', 'table_name': table_name, 'column_name': column_name}
    elif DB_TYPE == 'mysql':
        query = """
            SELECT COUNT(*) > 0 
            FROM information_schema.columns 
            WHERE table_schema = DATABASE() 
            AND table_name = :table_name 
            AND column_name = :column_name
        """
        params = {'table_name': table_name, 'column_name': column_name}
    else:
        # SQLite - more complex, using pragma
        query = f"PRAGMA table_info({table_name})"
        result = execute_sql(query)
        columns = [row[1] for row in result.fetchall()]  # column name is at index 1
        exists = column_name in columns
        logger.debug(f"Column {column_name} exists in {table_name}: {exists}")
        return exists
    
    result = execute_sql(query, params)
    exists = result.scalar()
    logger.debug(f"Column {column_name} exists in {table_name}: {exists}")
    return exists

def create_index_if_not_exists(index_name: str, table_name: str, columns: List[str], 
                               unique: bool = False, schema: Optional[str] = None) -> None:
    """Create an index if it doesn't already exist."""
    if DB_TYPE != 'postgresql':
        # For MySQL/SQLite, just create - they'll fail gracefully if exists
        if unique:
            op.create_unique_constraint(index_name, table_name, columns, schema=schema)
        else:
            op.create_index(index_name, table_name, columns, schema=schema)
        return
    
    # PostgreSQL specific check
    connection = get_connection()
    check_query = """
        SELECT COUNT(*) 
        FROM pg_indexes 
        WHERE indexname = :index_name 
        AND schemaname = COALESCE(:schema, 'public')
    """
    result = execute_sql(check_query, {'index_name': index_name, 'schema': schema})
    
    if result.scalar() == 0:
        if unique:
            op.create_unique_constraint(index_name, table_name, columns, schema=schema)
        else:
            op.create_index(index_name, table_name, columns, schema=schema)
        logger.info(f"Created {'unique ' if unique else ''}index {index_name} on {table_name}")
    else:
        logger.info(f"Index {index_name} already exists, skipping")

def drop_index_if_exists(index_name: str, table_name: Optional[str] = None, 
                         schema: Optional[str] = None) -> None:
    """Drop an index if it exists."""
    if DB_TYPE != 'postgresql':
        # For MySQL/SQLite, try to drop
        try:
            op.drop_index(index_name, table_name=table_name, schema=schema)
            logger.info(f"Dropped index {index_name}")
        except:
            logger.info(f"Index {index_name} didn't exist, skipping")
        return
    
    # PostgreSQL specific check
    connection = get_connection()
    check_query = """
        SELECT COUNT(*) 
        FROM pg_indexes 
        WHERE indexname = :index_name 
        AND schemaname = COALESCE(:schema, 'public')
    """
    result = execute_sql(check_query, {'index_name': index_name, 'schema': schema})
    
    if result.scalar() > 0:
        op.drop_index(index_name, table_name=table_name, schema=schema)
        logger.info(f"Dropped index {index_name}")
    else:
        logger.info(f"Index {index_name} doesn't exist, skipping")

def backup_table(table_name: str, backup_suffix: str = "_backup") -> Optional[str]:
    """Create a backup of a table before modification."""
    if is_production():
        logger.warning(f"Creating backup of {table_name} in production")
    
    backup_name = f"{table_name}{backup_suffix}"
    
    if table_exists(backup_name):
        logger.warning(f"Backup table {backup_name} already exists, skipping backup")
        return None
    
    try:
        execute_sql(f"CREATE TABLE {backup_name} AS SELECT * FROM {table_name}")
        logger.info(f"Created backup table {backup_name}")
        return backup_name
    except Exception as e:
        logger.error(f"Failed to create backup of {table_name}: {e}")
        return None

def log_migration_start() -> None:
    """Log migration start with metadata."""
    logger.info("=" * 80)
    logger.info(f"STARTING MIGRATION: {MIGRATION_NAME}")
    logger.info(f"Revision: {revision}")
    logger.info(f"Environment: {ENVIRONMENT}")
    logger.info(f"Database Type: {DB_TYPE}")
    logger.info(f"Generated at: {GENERATED_AT}")
    logger.info("=" * 80)
    
    if is_production():
        logger.warning("PRODUCTION ENVIRONMENT - PROCEED WITH CAUTION")
        logger.warning("Ensure you have verified this migration in staging")

def log_migration_end(success: bool = True, duration: float = None) -> None:
    """Log migration completion."""
    status = "SUCCESS" if success else "FAILED"
    logger.info("=" * 80)
    logger.info(f"MIGRATION {status}: {MIGRATION_NAME}")
    logger.info(f"Revision: {revision}")
    if duration:
        logger.info(f"Duration: {duration:.2f} seconds")
    logger.info("=" * 80)

## ==================== DATA MIGRATION HELPERS ====================
def seed_reference_data(table_name: str, data: List[Dict], 
                        id_column: str = 'id', 
                        update_existing: bool = False) -> None:
    """
    Seed reference data, optionally updating existing records.
    
    Args:
        table_name: Name of the table to seed
        data: List of dictionaries with data to insert
        id_column: Name of the ID column for conflict detection
        update_existing: Whether to update existing records
    """
    if not data:
        return
    
    connection = get_connection()
    metadata = sa.MetaData()
    metadata.reflect(bind=connection, only=[table_name])
    table_obj = metadata.tables[table_name]
    
    for record in data:
        if id_column in record:
            # Check if record exists
            check_query = sa.select([sa.text('1')]).where(
                table_obj.c[id_column] == record[id_column]
            )
            exists = connection.execute(check_query).fetchone() is not None
            
            if exists and update_existing:
                # Update existing record
                update_stmt = table_obj.update().where(
                    table_obj.c[id_column] == record[id_column]
                ).values(**record)
                connection.execute(update_stmt)
                logger.debug(f"Updated record in {table_name}: {record[id_column]}")
            elif not exists:
                # Insert new record
                insert_stmt = table_obj.insert().values(**record)
                connection.execute(insert_stmt)
                logger.debug(f"Inserted record into {table_name}: {record[id_column]}")
        else:
            # No ID column, just insert
            insert_stmt = table_obj.insert().values(**record)
            connection.execute(insert_stmt)
            logger.debug(f"Inserted record into {table_name}")

## ==================== TRANSACTION MANAGEMENT ====================
def run_in_transaction(func, *args, **kwargs):
    """Run a function within a transaction with proper error handling."""
    connection = get_connection()
    transaction = connection.begin()
    
    try:
        result = func(*args, **kwargs)
        transaction.commit()
        return result
    except Exception as e:
        transaction.rollback()
        logger.error(f"Transaction failed, rolled back: {e}")
        raise

## ==================== MIGRATION FUNCTIONS ====================
def upgrade() -> None:
    """Apply migration forward."""
    
    start_time = datetime.now(timezone.utc)
    log_migration_start()
    
    try:
        # Production safety check
        if is_production():
            # Add production-specific safety checks here
            logger.warning("Production migration - extra safety checks enabled")
        
        ${upgrades if upgrades else "# TODO: Add upgrade operations here"}
        
        # Log successful completion
        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        log_migration_end(success=True, duration=duration)
        
    except Exception as e:
        # Log failure
        logger.error(f"Migration failed: {e}", exc_info=True)
        log_migration_end(success=False)
        raise

def downgrade() -> None:
    """Revert migration backward."""
    
    start_time = datetime.now(timezone.utc)
    logger.warning("=" * 80)
    logger.warning("STARTING DOWNGRADE - DATA LOSS MAY OCCUR")
    logger.warning(f"Migration: {MIGRATION_NAME}")
    logger.warning(f"Revision: {revision}")
    logger.warning("=" * 80)
    
    try:
        # Extra warnings for production
        if is_production():
            logger.warning("PRODUCTION DOWNGRADE - THIS IS DANGEROUS!")
            logger.warning("Ensure you have a recent backup before proceeding")
        
        ${downgrades if downgrades else "# TODO: Add downgrade operations here"}
        
        # Log successful downgrade
        duration = (datetime.now(timezone.utc) - start_time).total_seconds()
        logger.warning("=" * 80)
        logger.warning(f"DOWNGRADE COMPLETE: {MIGRATION_NAME}")
        logger.warning(f"Duration: {duration:.2f} seconds")
        logger.warning("=" * 80)
        
    except Exception as e:
        logger.error(f"Downgrade failed: {e}", exc_info=True)
        logger.error("=" * 80)
        logger.error(f"DOWNGRADE FAILED: {MIGRATION_NAME}")
        logger.error("=" * 80)
        raise

## ==================== MIGRATION HOOKS ====================
def pre_upgrade() -> None:
    """Run before upgrade. Override if needed."""
    logger.info("Running pre-upgrade checks...")
    
    # Example: Ensure we have required tables
    required_tables = []
    for table in required_tables:
        if not table_exists(table):
            raise RuntimeError(f"Required table {table} does not exist")

def post_upgrade() -> None:
    """Run after upgrade. Override if needed."""
    logger.info("Running post-upgrade tasks...")
    
    # Example: Update statistics
    if DB_TYPE == 'postgresql':
        execute_sql("ANALYZE")
        logger.info("Updated database statistics")

def pre_downgrade() -> None:
    """Run before downgrade. Override if needed."""
    logger.warning("Running pre-downgrade checks...")
    
    # Example: Check for dependent data
    if is_production():
        logger.warning("Production downgrade - ensure you understand the impact")

def post_downgrade() -> None:
    """Run after downgrade. Override if needed."""
    logger.info("Running post-downgrade tasks...")

## ==================== MAIN EXECUTION GUARD ====================
if __name__ == "__main__":
    # This allows the migration to be run directly if needed
    # (not typically used, but useful for testing)
    print(f"This is an Alembic migration file: {MIGRATION_NAME}")
    print(f"Run with: alembic upgrade {revision}")
    sys.exit(0)

## ==================== AUTO-GENERATED OPERATIONS ====================
# The following code is auto-generated by Alembic when using --autogenerate
# It's preserved here for reference when manually creating migrations
<%text>
## Example operations (commented out):
# 
# ### Creating a table:
# op.create_table(
#     'new_table',
#     sa.Column('id', sa.Integer(), nullable=False),
#     sa.Column('name', sa.String(length=100), nullable=False),
#     sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()')),
#     sa.PrimaryKeyConstraint('id'),
#     sa.UniqueConstraint('name')
# )
# 
# ### Adding a column:
# op.add_column('table_name', sa.Column('new_column', sa.String(length=50)))
# 
# ### Modifying a column:
# op.alter_column('table_name', 'column_name',
#                 existing_type=sa.String(length=50),
#                 type_=sa.String(length=100),
#                 nullable=False)
# 
# ### Creating an index:
# op.create_index('idx_table_name_column', 'table_name', ['column_name'])
# 
# ### Bulk insert:
# op.bulk_insert(
#     table('table_name', 
#           column('id', sa.Integer),
#           column('name', sa.String)),
#     [
#         {'id': 1, 'name': 'Test 1'},
#         {'id': 2, 'name': 'Test 2'},
#     ]
# )
</%text>